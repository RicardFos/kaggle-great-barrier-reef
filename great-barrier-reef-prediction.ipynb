{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0.Install Tensorflow Object Detection API\nThis section follows the installation guide provided in the tensorflow object detection api github: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md","metadata":{}},{"cell_type":"code","source":"# clone tensorflow/models repository\n!git clone https://github.com/tensorflow/models","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:01:09.926718Z","iopub.status.busy":"2022-02-08T15:01:09.926158Z","iopub.status.idle":"2022-02-08T15:01:31.994163Z","shell.execute_reply":"2022-02-08T15:01:31.993360Z","shell.execute_reply.started":"2022-02-08T15:01:09.926678Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checkout a commit to ommit changes after this notebook\n!cd models && git checkout f08513d","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Go into models/research and check\n%cd models/research\n!dir","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:01:31.996592Z","iopub.status.busy":"2022-02-08T15:01:31.996373Z","iopub.status.idle":"2022-02-08T15:01:32.676314Z","shell.execute_reply":"2022-02-08T15:01:32.675527Z","shell.execute_reply.started":"2022-02-08T15:01:31.996565Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile protos\n!protoc object_detection/protos/*.proto --python_out=.","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:01:32.678283Z","iopub.status.busy":"2022-02-08T15:01:32.677764Z","iopub.status.idle":"2022-02-08T15:01:33.398390Z","shell.execute_reply":"2022-02-08T15:01:33.397446Z","shell.execute_reply.started":"2022-02-08T15:01:32.678239Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"setup_str = \"\"\"\nimport os\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = [\n    # Required for apache-beam with PY3\n    'avro-python3',\n    'apache-beam',\n    'pillow',\n    'lxml',\n    'matplotlib',\n    'Cython',\n    'contextlib2',\n    'tf-slim==1.1.0',\n    'six',\n    'pycocotools',\n    'lvis',\n    'scipy',\n    'pandas',\n    'tf-models-official==2.7.0',\n    'tensorflow_io==0.23.1',\n    'keras==2.7.0'\n]\n\nsetup(\n    name='object_detection',\n    version='0.1',\n    install_requires=REQUIRED_PACKAGES,\n    include_package_data=True,\n    packages=(\n        [p for p in find_packages() if p.startswith('object_detection')] +\n        find_packages(where=os.path.join('.', 'slim'))),\n    package_dir={\n        'datasets': os.path.join('slim', 'datasets'),\n        'nets': os.path.join('slim', 'nets'),\n        'preprocessing': os.path.join('slim', 'preprocessing'),\n        'deployment': os.path.join('slim', 'deployment'),\n        'scripts': os.path.join('slim', 'scripts'),\n    },\n    description='Tensorflow Object Detection Library',\n    python_requires='>3.6',\n)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('setup.py', 'w') as file:  # Use file to refer to the file object\n    file.write(setup_str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install TensorFlow Object Detection API.\n#!copy .\\object_detection\\packages\\tf2\\setup.py\n!python -m pip install .","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:01:33.402389Z","iopub.status.busy":"2022-02-08T15:01:33.402153Z","iopub.status.idle":"2022-02-08T15:02:59.610741Z","shell.execute_reply":"2022-02-08T15:02:59.609860Z","shell.execute_reply.started":"2022-02-08T15:01:33.402360Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the installation.\n!python object_detection/builders/model_builder_tf2_test.py","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:04:19.053767Z","iopub.status.busy":"2022-02-08T15:04:19.053543Z","iopub.status.idle":"2022-02-08T15:04:54.402052Z","shell.execute_reply":"2022-02-08T15:04:54.401151Z","shell.execute_reply.started":"2022-02-08T15:04:19.053733Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Go back to home directory\n%cd ../..\n!dir","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:05:11.166807Z","iopub.status.busy":"2022-02-08T15:05:11.166455Z","iopub.status.idle":"2022-02-08T15:05:11.989455Z","shell.execute_reply":"2022-02-08T15:05:11.988557Z","shell.execute_reply.started":"2022-02-08T15:05:11.166764Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#restart runtime to avoid bug when importing tensorflow\nimport os\nos.kill(os.getpid(), 9)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cv2\nimport io\nimport os\nimport ast\nimport tensorflow as tf","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:05:50.461014Z","iopub.status.busy":"2022-02-08T15:05:50.460526Z","iopub.status.idle":"2022-02-08T15:05:50.465317Z","shell.execute_reply":"2022-02-08T15:05:50.464637Z","shell.execute_reply.started":"2022-02-08T15:05:50.460976Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"execution":{"iopub.execute_input":"2022-02-08T15:05:51.531842Z","iopub.status.busy":"2022-02-08T15:05:51.531119Z","iopub.status.idle":"2022-02-08T15:05:51.662114Z","shell.execute_reply":"2022-02-08T15:05:51.660939Z","shell.execute_reply.started":"2022-02-08T15:05:51.531805Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Load the data","metadata":{}},{"cell_type":"code","source":"# Path\nINPUT_PATH = os.getcwd()\nIMAGES_PATH = os.path.join(INPUT_PATH, \"train_images\")\nprint(\"Images path: \", IMAGES_PATH)\ndf_train = pd.read_csv(os.path.join(INPUT_PATH,'train.csv'))\ndf_test = pd.read_csv(os.path.join(INPUT_PATH,'test.csv'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.path.join(IMAGES_PATH, \"video_\") ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"image_path\"] = IMAGES_PATH + \"/video_\" \\\n                        + df_train[\"video_id\"].astype(str) \\\n                        + \"/\" + df_train[\"video_frame\"].astype(str) + \".jpg\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# annotations are saved as an array formatted as a string (like \"[]\"), \n# I use the ast library to extract the array from the string (to get [] instead of \"[]\")\ndf_train[\"annotations\"] = df_train[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n# having the annotations in array format, I can now check the length of the array \n# getting how many bounding boxes are in the image\ndf_train[\"num_bboxes\"] = df_train[\"annotations\"].apply(lambda x: len(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load annotated image into numpy array and show using matplotlib","metadata":{"execution":{"iopub.execute_input":"2022-02-01T13:03:59.516545Z","iopub.status.busy":"2022-02-01T13:03:59.516171Z","iopub.status.idle":"2022-02-01T13:03:59.524441Z","shell.execute_reply":"2022-02-01T13:03:59.522485Z","shell.execute_reply.started":"2022-02-01T13:03:59.516512Z"}}},{"cell_type":"code","source":"annotaded_image_path = df_train[df_train[\"num_bboxes\"]>0][\"image_path\"].values[0]\nimage = cv2.imread(annotaded_image_path)\n#convert to RGB, cv2 cretaes BGR image by default\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nprint(\"Image shape: \", image.shape)\nplt.imshow(image)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load image using PIL.Image","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nImage.open(annotaded_image_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation_dict = df_train[df_train[\"num_bboxes\"]>0].head(1)[\"annotations\"].values[0]\nprint(annotation_dict[0])\nx_start = annotation_dict[0][\"x\"]\nx_end = x_start + annotation_dict[0][\"width\"]\ny_start = annotation_dict[0][\"y\"]\ny_end = y_start + annotation_dict[0][\"height\"]\n\nprint(\"bbox x_start: \", x_start)\nprint(\"bbox x_end: \", x_end)\nprint(\"bbox x_start: \", y_start)\nprint(\"bbox x_start: \", y_end)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw the bounding box in the image\nimage = cv2.imread(annotaded_image_path)\nstart_point = (x_start, y_start)\nend_point = (x_end, y_end)\n# Red color in BGR\ncolor = (0, 0, 255)\n# Line thickness of 2 px\nthickness = 2\nimage = cv2.rectangle(image, start_point, end_point, color, thickness)\n# Window name in which image is displayed\nwindow_name = 'Image'\n# Displaying the image \n#convert to format bgr\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nImage.fromarray(image, mode=\"RGB\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to draw the bounding boxes in any image\ndef show_image_with_bboxes(df_row):\n    annotations = df_row[\"annotations\"]\n    image = cv2.imread(df_row[\"image_path\"])\n    for annotation in annotations:\n        x_start = annotation[\"x\"]\n        x_end = x_start + annotation[\"width\"]\n        y_start = annotation[\"y\"]\n        y_end = y_start + annotation[\"height\"]\n        start_point = (x_start, y_start)\n        end_point = (x_end, y_end)\n        # Red color in BGR\n        color = (0, 0, 255)\n        # Line thickness of 2 px\n        thickness = 2\n        image = cv2.rectangle(image, start_point, end_point, color, thickness)\n        # Window name in which image is displayed\n    window_name = 'Image'\n    # Displaying the image\n    #convert to format bgr\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return Image.fromarray(image, mode=\"RGB\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row = df_train[df_train[\"num_bboxes\"]>9].head(1)\nexample = row.to_dict(orient='records')[0]\nshow_image_with_bboxes(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Details of the training Data\n-Frames per video\n<br>\n-Sequences per video\n<br>\n-Frames per sequence\n<br>\n-Annotations","metadata":{}},{"cell_type":"code","source":"info_videos = df_train.groupby(\"video_id\").agg({\"sequence\": pd.Series.nunique, \"video_frame\": \"count\"})\ninfo_videos = info_videos.rename(columns={\"sequence\": \"total_sequences\"})\ninfo_videos = info_videos.rename(columns={\"video_frame\": \"total_video_frames\"})\nprint(\"Number of sequences and frames in each video: \\n\")\ninfo_videos","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_frames = info_videos.sum()[\"total_video_frames\"]\ntotal_sequences = info_videos.sum()[\"total_sequences\"]\nprint(\"Total number of frames (number of images) = \", total_frames)\nprint(\"Total number of sequences = \", total_sequences)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ratio of the total frames in each video\ninfo_videos[\"ratio_video_frame\"] = (info_videos[\"total_video_frames\"]/total_frames)\nprint(\"Ratio of the total frames in each video\")\ninfo_videos = info_videos.reset_index()\ninfo_videos","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_sequences = (df_train.groupby([\"video_id\", \"sequence\"]).count()[\"sequence_frame\"]).to_frame()\nprint(\"Number of frames in each sequence: \\n\")\ninfo_sequences = info_sequences.rename(columns={\"sequence_frame\": \"total_sequence_frames\"})\ninfo_sequences","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ratio of the total frames in each sequence\ninfo_sequences[\"ratio_sequence_frame\"] = (info_sequences[\"total_sequence_frames\"]/total_frames)\nprint(\"Ratio of the total frames in each sequence\")\ninfo_sequences","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#number of annotations\ntotal_bounding_boxes = sum(df_train[\"num_bboxes\"])\ntotal_frames_with_bbox = len(df_train[df_train[\"num_bboxes\"] > 0][\"video_frame\"])\nprint(\"Total number of bounding boxes = \", total_bounding_boxes)\nprint(\"Total number of frames with at least a bounding box = \", total_frames_with_bbox)\nprint(\"Percentage of images with bounding boxes = \", (total_frames_with_bbox/total_frames)*100, \"%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_videos[\"num_bboxes\"] = df_train.groupby(\"video_id\").agg(pd.Series.sum)[\"num_bboxes\"]\ninfo_videos[\"ratio_video_bboxes\"] = info_videos[\"num_bboxes\"] / total_bounding_boxes\ninfo_videos[\"frames_with_bbox\"] = df_train[df_train[\"num_bboxes\"] > 0].groupby(\"video_id\").agg(pd.Series.count)[\"video_frame\"]\ninfo_videos[\"ratio_video_frames_with_bboxes\"] = info_videos[\"frames_with_bbox\"] / total_frames_with_bbox\ninfo_videos","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(\"video_\"+info_videos[\"video_id\"].astype(str), info_videos[\"total_sequences\"])\nplt.ylabel(\"sequences\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(\"video_\"+info_videos[\"video_id\"].astype(str), info_videos[\"total_video_frames\"])\nplt.ylabel(\"frames\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(\"video_\"+info_videos[\"video_id\"].astype(str), info_videos[\"frames_with_bbox\"])\nplt.ylabel(\"frames with bbox\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(\"video_\"+info_videos[\"video_id\"].astype(str), info_videos[\"num_bboxes\"])\nplt.ylabel(\"num bounding boxes\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_sequences[\"num_bboxes\"] = df_train.groupby([\"video_id\", \"sequence\"]).agg(pd.Series.sum)[\"num_bboxes\"]\ninfo_sequences[\"ratio_sequence_bboxes\"] = info_sequences[\"num_bboxes\"] / total_bounding_boxes\ninfo_sequences[\"frames_with_bbox\"] = df_train[df_train[\"num_bboxes\"] > 0].groupby([\"video_id\", \"sequence\"]).agg(pd.Series.count)[\"video_frame\"]\ninfo_sequences[\"frames_with_bbox\"] = info_sequences[\"frames_with_bbox\"].fillna(0).astype(int)\ninfo_sequences[\"ratio_sequence_frames_with_bboxes\"] = info_sequences[\"frames_with_bbox\"] / total_frames_with_bbox\ninfo_sequences = info_sequences.reset_index()\ninfo_sequences","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#info_sequences.loc[[info_sequences[\"num_bboxes\"].idxmax()]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(info_sequences[\"sequence\"].astype(str), info_sequences[\"total_sequence_frames\"])\nplt.ylabel(\"frames\")\nplt.xlabel(\"sequences\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(info_sequences[\"sequence\"].astype(str), info_sequences[\"frames_with_bbox\"])\nplt.ylabel(\"frames with bbox\")\nplt.xlabel(\"sequences\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(info_sequences[\"sequence\"].astype(str), info_sequences[\"num_bboxes\"])\nplt.ylabel(\"Bounding boxes\")\nplt.xlabel(\"sequences\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.Train - Eval split\nI choose to split train and evaluation sets the following way:\n- A single sequence can't be split between training and evaluation set, as images are similar and would leak information\n- Keeping around 75% to 85% of the total images with at least a bounding box in the training set\n- Keeping around 75% to 85% of the total number of images in the training set\n\nLooking at info_videos and info_sequences dataframes, I've chosen to keep all images from video 0 and video 1 and images from the sequence 37114 of the video 2 in the training set. That way, the training set has around 86% of the images with bounding boxes and around 75% of the total number of images. It will have around 79% of the total number of bounding boxes as well.","metadata":{}},{"cell_type":"code","source":"# placeholder, finish in future versions to obtain random splits\nfrom sklearn.model_selection import train_test_split\n\ndef split_sequences(sequences_info_df, train_ratio):\n    sequence_list = sequences_info_df[\"sequence\"].values\n    train, test = train_test_split(sequence_list, test_size = train_ratio)\n    return (train, test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_train_split = info_sequences[(info_sequences[\"video_id\"] == 0) | (info_sequences[\"video_id\"] == 1) | (info_sequences[\"sequence\"] == 37114)]\nsequences_train_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training set split:\")\nprint(\"percentage of images with bounding boxes: \", sum(sequences_train_split[\"ratio_sequence_frames_with_bboxes\"])*100, \"%\")\nprint(\"percentage of total images: \", sum(sequences_train_split[\"ratio_sequence_frame\"])*100, \"%\")\nprint(\"percentage of bounding boxes: \", sum(sequences_train_split[\"ratio_sequence_bboxes\"])*100, \"%\")\ntotal_split_train_images = sum(sequences_train_split[\"total_sequence_frames\"])\ntotal_split_eval_images = len(df_train) - total_split_train_images\nprint(\"Total images in training split: \", total_split_train_images)\nprint(\"Total images in evaluation split: \", total_split_eval_images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_split = df_train[df_train[\"sequence\"].isin(sequences_train_split[\"sequence\"])]\ndf_train_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_eval_split = df_train[~df_train[\"sequence\"].isin(sequences_train_split[\"sequence\"])]\ndf_eval_split = df_eval_split.reset_index()\ndf_eval_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.Create TF records\nThis section follows the official guide \"Bringing in your own dataset\" from TF object detection github: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md","metadata":{}},{"cell_type":"code","source":"from object_detection.utils import dataset_util, label_map_util\n\ndef create_tf_example(example):\n    image = Image.open(example[\"image_path\"], mode='r')\n    encoded_image_data = io.BytesIO()\n    image.save(encoded_image_data, image.format)\n    encoded_image_data = encoded_image_data.getvalue()  # Encoded image byte\n    #print(\"encoded_image_data: \", encoded_image_data)\n    width, height = image.size\n#     print(\"width: \", width)\n#     print(\"height: \", height)\n    filename = example[\"image_path\"].encode('utf8') # Filename of the image. Empty if image is not from file\n#     print(\"filename: \", filename)\n    image_format = b'jpg'\n    class_name = \"COT\"\n    class_name = class_name.encode('utf8')\n    \n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n    \n    for annotation in example[\"annotations\"]:\n        xmins.append(annotation[\"x\"]/width)\n        xmaxs.append((annotation[\"x\"]+annotation[\"width\"])/width)\n        ymins.append(annotation[\"y\"]/height)\n        ymaxs.append((annotation[\"y\"]+annotation[\"height\"])/height)\n        classes_text.append(class_name)\n        classes.append(1)\n        \n#     print(\"xmins: \", xmins)\n#     print(\"xmaxs: \", xmaxs)\n#     print(\"ymins: \", ymins)\n#     print(\"ymaxs: \", ymaxs)\n#     print(\"classes_text: \", classes_text)\n#     print(\"classes: \", classes)\n    \n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(filename),\n      'image/source_id': dataset_util.bytes_feature(filename),\n      'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n      'image/format': dataset_util.bytes_feature(image_format),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    return tf_example\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tf_record(df, record_path):\n    writer = tf.python_io.TFRecordWriter(record_path)\n    for index, example in df.iterrows():\n        tf_example = create_tf_example(example)\n        writer.write(tf_example.SerializeToString())\n    writer.close()\n    print('Successfully created the TFRecord file: {}'.format(record_path))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contextlib2\nfrom object_detection.dataset_tools import tf_record_creation_util\n\ndef create_tf_record_shards(df, output_filebase, num_shards=10):\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n              tf_record_close_stack, output_filebase, num_shards)\n        for index, example in df.iterrows():\n            tf_example = create_tf_example(example)\n            output_shard_index = index % num_shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir training","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CURRENT_DIRECTORY = os.getcwd()\nTRAINING_DIRECTORY = os.path.join(CURRENT_DIRECTORY, \"training\")\nTRAIN_RECORD_PATH = os.path.join(TRAINING_DIRECTORY, \"train.record\")\nEVAL_RECORD_PATH = os.path.join(TRAINING_DIRECTORY, \"eval.record\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create training and evaluation tfrecords","metadata":{}},{"cell_type":"code","source":"#training tfrecord\ncreate_tf_record_shards(df_train_split, TRAIN_RECORD_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation tfrecord\ncreate_tf_record_shards(df_eval_split, EVAL_RECORD_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print an example from a record to check\nraw_dataset=tf.data.TFRecordDataset(TRAIN_RECORD_PATH+\"-00009-of-00010\")\nfor raw_record in raw_dataset.take(3):\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    print(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # show image with bounding boxes from the example above, looking at the video_0 and folder and frame_id from the filename\nrow = df_train[(df_train[\"video_id\"]==0) & (df_train[\"video_frame\"]==29)]\nexample = row.to_dict(orient='records')[0]\nshow_image_with_bboxes(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create the label map","metadata":{}},{"cell_type":"code","source":"LABELMAP_PATH = os.path.join(TRAINING_DIRECTORY, \"label_map.pbtxt\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create label map\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COT'\n}\"\"\"\n# Write labelmap\nwith open(LABELMAP_PATH, 'w') as writefile:\n    writefile.write(label_map_str)\n!more {LABELMAP_PATH}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.Training the model\nThis section follows the official guide \"Bringing in your own dataset\" from TF object detection github: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md","metadata":{}},{"cell_type":"code","source":"# Download the pretrained EfficientDet-D0 model\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir cot_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COT_MODEL_DIRECTORY = os.path.join(CURRENT_DIRECTORY, \"cot_model\")\nPIPELINE_CONFIG_PATH = os.path.join(COT_MODEL_DIRECTORY, \"cot_model.config\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_txt= \"\"\"\n# SSD with EfficientNet-b0 + BiFPN feature extractor,\n# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n# See EfficientDet, Tan et al, https://arxiv.org/abs/1911.09070\n# See Lin et al, https://arxiv.org/abs/1708.02002\n# Trained on COCO, initialized from an EfficientNet-b0 checkpoint.\n#\n# Train on TPU-8\n\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    add_background_class: false\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: [1.0, 2.0, 0.5]\n        scales_per_octave: 3\n      }\n    }\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 512\n        max_dimension: 512\n        pad_to_max_dimension: true\n        }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        depth: 64\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          force_use_bias: true\n          activation: SWISH\n          regularizer {\n            l2_regularizer {\n              weight: 0.00004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            scale: true\n            decay: 0.99\n            epsilon: 0.001\n          }\n        }\n        num_layers_before_predictor: 3\n        kernel_size: 3\n        use_depthwise: true\n      }\n    }\n    feature_extractor {\n      type: 'ssd_efficientnet-b0_bifpn_keras'\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 3\n        num_filters: 64\n      }\n      conv_hyperparams {\n        force_use_bias: true\n        activation: SWISH\n        regularizer {\n          l2_regularizer {\n            weight: 0.00004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          scale: true,\n          decay: 0.99,\n          epsilon: 0.001,\n        }\n      }\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 1.5\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\n\ntrain_config: {\n  fine_tune_checkpoint: \"efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint_type: \"detection\"\n  batch_size: 2\n  sync_replicas: true\n  startup_delay_steps: 0\n  replicas_to_aggregate: 1\n  use_bfloat16: true\n  num_steps: 10000\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 512\n      scale_min: 0.1\n      scale_max: 2.0\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 8e-2\n          total_steps: 10000\n          warmup_learning_rate: .001\n          warmup_steps: 1000\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\n\ntrain_input_reader: {\n  label_map_path: \"training/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"training/train.record-?????-of-00010\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1;\n}\n\neval_input_reader: {\n  label_map_path: \"training/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"training/eval.record-?????-of-00010\"\n  }\n}\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(PIPELINE_CONFIG_PATH, 'w') as config_file:\n    config_file.write(config_txt)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!more {PIPELINE_CONFIG_PATH}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training the model, use cmd terminal instead","metadata":{}},{"cell_type":"code","source":"# train model\n!python models/research/object_detection/model_main_tf2.py \\\n  --model_dir=squeal_model \\\n  --pipeline_config_path=squeal_model/squeal_model.config \\\n  --alsologtostderr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model\n!python models/research/object_detection/model_main_tf2.py \\\n  --model_dir=cot_model \\\n  --pipeline_config_path=cot_model/cot_model.config \\\n  --checkpoint_dir=cot_model \\\n  --eval_timeout=0 \\\n  --alsologtostderr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Export model to get saved model","metadata":{}},{"cell_type":"code","source":"!mkdir saved_models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd saved_models\n!mkdir cot_model\n%cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python models/research/object_detection/exporter_main_v2.py --input_type image_tensor \\\n  --pipeline_config_path=cot_model/cot_model.config \\\n  --trained_checkpoint_dir=cot_model \\\n  --output_directory=saved_models/cot_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd saved_models\n!tar.exe -a -c -f cot_model.zip cot_model\n%cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unzip saved models if they are imported\n%cd saved_models\n!tar -xvzf cot_model_highres.zip\n%cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate saved model on console:\n\npython models/research/object_detection/model_main_tf2.py --model_dir=saved_models/cot_model/checkpoint --pipeline_config_path=saved_models/cot_model_v4/pipeline.config --checkpoint_dir=saved_models/cot_model_v4/checkpoint --eval_timeout=0 --alsologtostderr\n","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"SAVED_MODEL_DIRECTORY = os.path.join(CURRENT_DIRECTORY, \"saved_models\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the TensorFlow COTS detection model into memory.\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(SAVED_MODEL_DIRECTORY, 'cot_model_v4'), 'saved_model'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from six import BytesIO\ndef load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: the file path to the image\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(BytesIO(img_data))\n    (im_width, im_height) = image.size\n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row = df_train_split[df_train_split[\"num_bboxes\"] >0].head(1)\nexample = row.to_dict(orient='records')[0]\nrow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = example[\"image_path\"]\nimage_np = load_image_into_numpy_array(image_path)\ninput_tensor = np.expand_dims(image_np, 0)\ndetections = detect_fn_tf_odt(input_tensor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detections","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from object_detection.utils import visualization_utils as viz_utils\n\ndef show_image_with_detected_bboxes(example):\n    image_path = example[\"image_path\"]\n    image_np = load_image_into_numpy_array(image_path)\n    input_tensor = np.expand_dims(image_np, 0)\n    detections = detect_fn_tf_odt(input_tensor)\n    category_index = label_map_util.create_category_index_from_labelmap(LABELMAP_PATH,\n                                                                    use_display_name=True)\n    # All outputs are batches tensors.\n    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n    # We're only interested in the first num_detections.\n    num_detections = int(detections.pop('num_detections'))\n    detections = {key: value[0, :num_detections].numpy()\n                   for key, value in detections.items()}\n    detections['num_detections'] = num_detections\n\n    # detection_classes should be ints.\n    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n    \n    image_np_with_detections = image_np.copy()\n\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n          image_np_with_detections,\n          detections['detection_boxes'],\n          detections['detection_classes'],\n          detections['detection_scores'],\n          category_index,\n          use_normalized_coordinates=True,\n          max_boxes_to_draw=200,\n          min_score_thresh=.10,\n          agnostic_mode=False)\n    return Image.fromarray(image_np_with_detections, mode=\"RGB\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_with_detected_bboxes(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_with_bboxes(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row = df_eval_split[df_eval_split[\"num_bboxes\"] >1].head(1)\nexample = row.to_dict(orient='records')[0]\nrow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = example[\"image_path\"]\nimage_np = load_image_into_numpy_array(image_path)\ninput_tensor = np.expand_dims(image_np, 0)\ndetections = detect_fn_tf_odt(input_tensor)\ndetections","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_with_detected_bboxes(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_with_bboxes(example)","metadata":{},"execution_count":null,"outputs":[]}]}